\section{Current research on sequence labeling}
To search relevant studies about sequence labeling that could be applied in the data carving field, the following research questions were defined:

\begin{enumerate}
\item   What are the sequence labeling algorithms used to treat unstructured data?
\item   In which application areas they are used?
\item   How these algorithms compare to each other?
\item   Which of them could produce better results in data carving?
\end{enumerate}

% The search string is expected to provide a direct answer to the first question, while the remaining ones should be answered by each algorithm description.

To conduct the search, the following digital libraries were used: 
\begin{itemize}
\item{ACM} (https://dl.acm.org/)
\item IEEE (https://ieeexplore.ieee.org/)
\item Scopus (https://www.scopus.com/) 
\item Springer Link (https://link.springer.com/)
\end{itemize}

The terms chosen to address those questions are shown in table \ref{tab:terms}, grouped by research structure using the boolean operation “OR”, then globally combined using the boolean operation “AND”, resulting in the search string presented in  Figure \ref{fig:searchstring}.

To choose these terms, the abbreviations of some common techniques applied in sequence labeling were used, assuming it would be unlikely that a new technique would not mention one of these.

\begin{table*}[!ht]
    \centering
    \begin{tabular}{ l  l  }
      Structure 	& Terms 		 \\
      \hline\hline
      Population 	& unstructured data \\   
                    & sequential data \\
      \hline
      Intervention 	& CNN \\
                    & RNN \\
                    & HMM \\
                    & CRF \\
                    & MEMM \\
                    & DTW \\
      \hline
      Outcome 		& sequence labeling \\
      \hline
    \end{tabular}
    \caption{Terms used}
    \label{tab:terms}
\end{table*}

\begin{figure}[!ht]
  \centering
  \fbox{\parbox{\textwidth}{
    ("unstructured data"
    OR "sequential data"
    ) AND (
    "CNN"
    OR "RNN"
    OR "HMM"
    OR "CRF"
    OR "MEMM"
    OR "DTW"
    ) AND (
    "sequence labeling"
    )
  }}
  \caption{Search string}
  \label{fig:searchstring}
\end{figure}
	
The 197 returned papers were retrieved through the submission of the search string to the four mentioned databases.

Results with unavailable content were ignored.

With the exclusion of 6 duplicated studies, the title and the abstract of 191 were read and 68 
were accepted in the intermediary selection,
which required enough detail to implement the solution,
some form of technique improvement, and possibility of application on the data carving field.

Next, 47 of the papers were excluded after quality assessment, focused on eliminate redundant studies, resulting in 21 selected studies.

Table 
\ref{tab:results}
shows the total remaining studies from each database.


\begin{table}[h]
    \begin{tabular}{l r r r r}
    \hline
    Database &        Retrieved & 
                                Not duplic. & 
                                         Interm. Selection & 
                                               Final Selection\\
    \hline
    ACM	DL			& 40 &      40 &     12  & 5 \\
    IEEE Explore	& 72 &      72 &     24  & 7 \\
    Scopus			& 37 &      31 &     22  & 6 \\
    Springer Link	& 48 &      48 &     10  & 3 \\
    \hline
    Total           & 197 &    191 &     68  & 21 \\
    \hline
    
    \end{tabular}
    \caption{Search engines and selected studies at each phase.}\label{tab:results}
\end{table}

The conclusions drawn from those studies are presented next, answering the formulated questions:

\begin{enumerate}
\item   \textbf{What are the sequence labeling algorithms used to treat unstructured data?}

Two groups of algorithms were identified as the most cited in sequence labeling: conditional random fields (CRF) and bidirectional long short-term memory (BLSTM). 
Other algorithms found were hidden Markov models (HMM), to which CRF is related, and some neural networks variants, from which BLSTM is a subgroup. 
Some algorithms are variations of others and some are hybrids, such as the combination of CRF and neural networks, and some were used only as baselines.

From the selected studies,
3 of them offers a broad overview of sequence labeling or machine learning: 
\begin{itemize}
        \item 
    Graves \cite{gravessupervised} provides an excellent overview of sequence labeling, 
        \item 
    Deng and Li \cite{6423821} offers an overview of machine learning in general, which can be used to situate sequence labeling among other problems and to compare different techniques that have different goals and assumptions, 
        \item 
    Schmidhuber \cite{Schmidhuber201585} gives a detailed historical overview of machine learning.
\end{itemize}

The only selected study that do not apply CRF or neural networks was:
    \begin{itemize}
        \item 
    Daf{\'e} et al. \cite{dafe2015learning}, describing the approximately-contiguous sequential classifier (AC-SC) and pattern silhouettes, comparing it to HMM and other algorithms.
\end{itemize}

        
Conditional random fields (CRF) without neural networks are discussed in 
6 selected studies:
\begin{itemize}
        \item 
    Cuong et al. \cite{Cuong:2014:CRF:2627435.2638567} discusses high-order dependencies in CRF,
        \item 
    Chatzis and Demiris \cite{Chatzis20131523} discusses infinite order CRF, 
        \item 
    Wang and Zhang \cite{6213103} describes Hidden Conditional Random Fields (HCRF), 
        \item 
    Song et al. \cite{6247918} describes Multi-View Latent variable Discriminative Conditional Random Fields (MV-LDCRF),
        \item 
    Papadopoulos and Tzanetakis \cite{7579173} compares HMM and CRF,
        \item 
    Jeong and Lee \cite{4599397} explains triangular-chain CRF.
\end{itemize}

Neural Networks were used in 
11 of the selected studies.
From these, 
only 
4 do not use Long Short-Term Memory (LSTM): 
\begin{itemize}
        \item 
    Irsoy and Cardie \cite{Irsoy2014720} compares CRF to RNN,
        \item 
    Zhang et al. \cite{Zhang:2017:NNS:3105654.3105669} describes compact feedforward sequential memory network (cFSMN), 
        \item 
    Zhao et al. \cite{7953168} describes recurrent convolutional neural network (RCNN) and compares it with LSTM. 
        \item 
    Cui et al. \cite{Cui20162474} describes the hierarchical temporal memory (HTM) sequence memory model, a neural network model that can do continuous online learning, and compares it with LSTM and other techniques,
\end{itemize}

The remaining 
7 studies use LSTM, where 
\begin{itemize}
        \item 
    Nguyen et al. \cite{nguyen2018recurrent} compares CRF to some combined models of CRF and BLSTM,
        \item 
    Xie et al. \cite{7900261} describes fully convolutional recurrent network (FCRN) combining a convolutional network with a multi-layer BLSTM,
        \item 
    Fang et al. \cite{Fang:2017:DEU:3131672.3131693} describes hierarchical bidirectional deep recurrent neural network (HB-RNN), which uses a tree of BLSTM networks,
        \item 
    Veli\v{c}kovi\'{c} et al. \cite{Velickovic:2018:CRM:3240925.3240937} describes cross-modal LSTM (X-LSTM), which is a network of LSTMs,
        \item 
    Tjandra et al. \cite{Tjandra2016448} combines LSTM with Gated Recurrent Unit (GRU) using tensor products,
        \item 
    Liu et al. \cite{Liu20173671} proposes the global context aware attention LSTM (GCA-LSTM), that adds a global context memory block to LSTM, 
        \item 
    Feng et al. \cite{feng2018attention} adds a context attention layers to a hierarchical LSTM to produce a Context Attention LSTM (CA-LSTM).
\end{itemize}

\item   \textbf{In which application areas they are used?}

On the selected studies, these are some frequently seen areas in which sequence label was applied:

\begin{itemize}
    \item Part Of Speech (POS) tagging
    \cite{Chatzis20131523}
    \cite{Tjandra2016448}
    \unskip:
    associates part of speech labels to words in a sentence.
    
    \item Speech recognition
    \cite{Zhang:2017:NNS:3105654.3105669}
    \cite{7953168}
    \cite{gravessupervised}
    \cite{6423821}
    \unskip:
    converts spoken words into text.
    
    \item Handwriting recognition
    \cite{Cuong:2014:CRF:2627435.2638567}
    \cite{Chatzis20131523}
    \cite{7900261}
    \unskip:
    digitally transcribes a handwritten text.
    
    \item Protein sequence analysis
    \cite{dafe2015learning}
    \unskip:
    compare regions of DNA or RNA with known protein sequences.
    
    \item Action recognition
    \cite{6247918}
    % \item video segmentation
    \cite{Chatzis20131523}
    % \item video event classification
    \cite{6213103}
    % \item Gesture recognition:
    \cite{Liu20173671}
    % \item sign language
    \cite{Fang:2017:DEU:3131672.3131693}
    \unskip:
    gives labels to predefined sequences of human motion, commonly translating body postures and hand shapes to commands or words.  
    
    \item Information extraction
    % \item bibliography extraction
    \cite{Cuong:2014:CRF:2627435.2638567}
    % \item opinion mining
    \cite{Irsoy2014720}
    % \item legal texts
    \cite{nguyen2018recurrent}
    % sentiment classification
    \cite{feng2018attention}
    \cite{dafe2015learning}
    \unskip:
    retrieves structured information from unstructured data, often text.
    
\end{itemize}

\item   \textbf{How these algorithms compare to each other?}

\begin{itemize}
    \item HMM and CRF: 
    
    As explained by Papadopoulos and Tzanetakis \cite{7579173}, HMM and CRF are both cases of Markov networks, but differ in these aspects:
    \begin{itemize}
        \item HMM represents $p(y,x)$ while CRF represents $p(y|x)$ (where $x$ is an observation and $y$ the label), which is why HMM is considered a generative model and CRF a discriminative one.
        Graves \cite{gravessupervised} clarifies that $p(x)$, which can be obtained by $p(x) = \sum_{y} p(x,y)$, can be used to generate artificial data, hence the name.
        \item HMM assumes strong independence between observation variables while CRF relaxes that assumption.
        \item CRF generally gives better results than HMM in many real-world problems where the dependence between observation variables is common, like natural language processing, music audio signal processing, speech recognition, and bioinformatics.
        
    \end{itemize}
    
    \item High-order CRF:
    
    As Cuong et al. \cite{Cuong:2014:CRF:2627435.2638567} details, the common linear-chain CRF considers dependencies between at most two adjacent labels, because its computational cost is exponential in respect to the number of considered neighbors.
    
    Cuong et al. \cite{Cuong:2014:CRF:2627435.2638567} approach to build high-order dependency relies on label sparsity, which happens when a sequence of repeated labels is a common occurrence.
    
    To achieve higher order dependency, Chatzis and Demiris \cite{Chatzis20131523} combines CRF with a sequence memoizer, which is a non-Markovian model since it can consider all previous sequence values.
    
    % Wang and Zhang \cite{6213103} describes Hidden Conditional Random Fields (HCRF), 
    % Song et al. \cite{6247918} describes Multi-View Latent variable Discriminative Conditional Random Fields (MV-LDCRF),
    % Jeong and Lee \cite{4599397} explain triangular-chain CRF.
    
    
    \item CRF and neural networks:
    
    Irsoy and Cardie \cite{Irsoy2014720} compared CRF and RNN, obtaining better F1-scores with RNN.
    
    Nguyen et al. \cite{nguyen2018recurrent} compared CRF to some combined models of CRF and BLSTM, obtaining better F1-scores with the BLSTM-CRF models.

    \item Recurrent neural networks (RNN) and LSTM:
    
    As Graves \cite{gravessupervised} explains, is very difficult for a standard RNN to retain information for longs periods of time because, with time, new data from the input layer tends to prevail over signals from the hidden layer, while LSTM has special `memory cells', which can retain information as long as necessary.
    
    Graves \cite{gravessupervised} compared BLSTM, bidirectional recurrent neural network (BRNN) and multilayer perceptron (MLP). Compared with the other two, BLSTM produced better results in less time.
    
    Zhang et al. \cite{Zhang:2017:NNS:3105654.3105669} 
    proposes a nonrecurrent neural network model with a memory block and obtains better results in less time compared to BLSTM.

    Zhao et al. \cite{7953168} 
    reports that RNNs are hard to train because they cannot take full advantage of GPUs, while convolutional neural networks (CNN) does not improve speech recognition results compared to other techniques.
    Comparing the proposed model RCNN with LSTM, RCNN was faster to train and showed a slightly better result.

\end{itemize}




% discriminative vs generative classifiers

% segmentation problem

    \item   \textbf{Which of them could produce better results in data carving?}
    
    To answer that question first is necessary to define how the data carving problem compares to the usual problems that sequence labeling solves. In part-of-speech (POS) tagging, a limitation on how far in the sequence an algorithm is allowed to go back has a smaller impact when compared to speech recognition because the later has to solve segmentation, while words are already the desired segments. In speech recognition, each chunk of sound may be a new phoneme, or a continuation of the previous phoneme, or a pause, causing a larger dependency on older sequence items to improve results.
    Regarding the segmentation issue, a data carving algorithm faces a challenge similar to speech recognition, and also protein sequence analysis, for two reasons: its recognition patterns may have variable size and, due to fragmentation, there may exist large distances between related disk sectors holding the content of a deleted file.
    
    Merge comparisons from different studies is a complex task because they naturally use the plain model as a baseline. For example, there are studies achieving better results with high order CRFs instead of CRF, and studies promoting LSTM over CRF, but it is unclear how a higher order CRF compare to LSTM.
    
    Despite that difficulty, there seems to exist a general tendency giving BLSTM an advantage over CRF in speech recognition, while for natural language problems that gap may be smaller and CRF may be a good choice.
    
    While there are studies that reported improvements over some form of general algorithm baseline, it is unwise to use one of these as first attempts while planning a new work. It may happen that the improvements are not achieved when ported to another problem. Or unanticipated implementation issues may arise. Or the solution may not scale well. For that reason, even knowing that there are studies reporting better results than BLSTM, the later still may be the best choice for data carving.
\end{enumerate}
