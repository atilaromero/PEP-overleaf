\section{Current research on sequence labeling}
The employed background research methodology was inspired by PICO (Population, Intervention, Comparison, Outcome) criteria \cite{Kitchenham07guidelinesfor}.

To search relevant studies about sequence labeling that could be applied in the data carving field, the following research questions were defined:

\begin{enumerate}[itemindent=\parindent,label=\textbf{RQ\arabic*.}]
\item   What are the sequence labeling algorithms used to treat unstructured data?
\item   In which application areas they are used?
\item   How these algorithms compare to each other?
\item   Which of them could produce better results in data carving?
\end{enumerate}

To conduct the search, the following digital libraries were used: ACM (https://dl.acm.org/), 
IEEE (https://ieeexplore.ieee.org/),
Scopus (https://www.scopus.com/),
and
Springer Link (https://link.springer.com/).

The terms chosen to address those questions are shown in table \ref{tab:terms}, grouped by research structure using the boolean operation “OR”, then globally combined using the boolean operation “AND”, resulting in the search string presented in  Figure \ref{fig:searchstring}.

To choose these terms, the abbreviations of some common techniques applied in sequence labeling were used, assuming it would be unlikely that a new technique would not mention one of these.

\begin{table*}[!ht]
    \centering
    \begin{tabular}{ l  l  }
      Structure 	& Terms 		 \\
      \hline\hline
      Population 	& unstructured data \\   
                    & sequential data \\
      \hline
      Intervention 	& CNN \\
                    & RNN \\
                    & HMM \\
                    & CRF \\
                    & MEMM \\
                    & DTW \\
      \hline
      Outcome 		& sequence labeling \\
      \hline
    \end{tabular}
    \caption{Terms used}
    \label{tab:terms}
\end{table*}

\begin{figure}[!ht]
  \centering
  \fbox{\parbox{\textwidth}{
    ("unstructured data"
    OR "sequential data"
    ) AND (
    "CNN"
    OR "RNN"
    OR "HMM"
    OR "CRF"
    OR "MEMM"
    OR "DTW"
    ) AND (
    "sequence labeling"
    )
  }}
  \caption{Search string}
  \label{fig:searchstring}
\end{figure}
	
The submission of the search string to the four mentioned databases returned 197 papers.
Results with unavailable content were ignored.

With the exclusion of 6 duplicated studies, title and abstract of 191 of them were read, resulting in 68 accepted papers in the intermediary selection, which required enough detail to implement the solution, some form of technique improvement, and possibility of application on the data carving field.

Next, 47 of the papers were excluded after quality assessment, focused on eliminate redundant studies, resulting in 21 selected studies.

Table 
\ref{tab:results}
shows the total remaining studies from each database.


\begin{table}[h]
    \begin{tabular}{l r r r r}
    \hline
    Database &        Retrieved & 
                                Not duplic. & 
                                         Interm. Selection & 
                                               Final Selection\\
    \hline
    ACM	DL			& 40 &      40 &     12  & 5 \\
    IEEE Explore	& 72 &      72 &     24  & 7 \\
    Scopus			& 37 &      31 &     22  & 6 \\
    Springer Link	& 48 &      48 &     10  & 3 \\
    \hline
    Total           & 197 &    191 &     68  & 21 \\
    \hline
    
    \end{tabular}
    \caption{Search engines and selected studies at each phase.}\label{tab:results}
\end{table}

The conclusions drawn from those studies are presented next, answering the formulated questions:

\begin{enumerate}[itemindent=\parindent,label=\textbf{RQ\arabic*.}]
\item   \textbf{What are the sequence labeling algorithms used to treat unstructured data?}

Two groups of algorithms were identified as the most cited in sequence labeling: conditional random fields (CRF) and bidirectional long short-term memory (BLSTM). 
Other algorithms found were hidden Markov models (HMM), to which CRF is related, and some neural networks variants, from which BLSTM is a subgroup. 
Some algorithms are variations of others and some are hybrids, such as the combination of CRF and neural networks, and some were used only as baselines.

From the selected studies,
3 of them offers a broad overview of sequence labeling or machine learning: Graves \cite{gravessupervised}, Deng and Li \cite{6423821}, and Schmidhuber \cite{Schmidhuber201585}.
    Graves \cite{gravessupervised} provides an excellent overview of sequence labeling.
    Deng and Li \cite{6423821} offers an overview of machine learning in general, which can be used to situate sequence labeling among other problems and to compare different techniques that have different goals and assumptions. 
    Schmidhuber \cite{Schmidhuber201585} gives a detailed historical overview of machine learning.

The only selected study that do not apply CRF or neural networks was Daf{\'e} et al. \cite{dafe2015learning}, describing the approximately-contiguous sequential classifier (AC-SC) and pattern silhouettes, comparing it to HMM and other algorithms.

    
Conditional random fields (CRF) without neural networks are discussed in 
6 selected studies:
    % Cuong et al. \cite{Cuong:2014:CRF:2627435.2638567},
    % Chatzis and Demiris \cite{Chatzis20131523},
    % Wang and Zhang \cite{6213103}, Song et al. \cite{6247918},
    % Papadopoulos and Tzanetakis \cite{7579173},
    % and
    % Jeong and Lee \cite{4599397}.
    Cuong et al. \cite{Cuong:2014:CRF:2627435.2638567} discusses high-order dependencies in CRF,
    Chatzis and Demiris \cite{Chatzis20131523} discusses infinite order CRF,
    Wang and Zhang \cite{6213103} describes Hidden Conditional Random Fields (HCRF),
    Song et al. \cite{6247918} describes Multi-View Latent variable Discriminative Conditional Random Fields (MV-LDCRF),
    Papadopoulos and Tzanetakis \cite{7579173} compares HMM and CRF,
    and
    Jeong and Lee \cite{4599397} explains triangular-chain CRF.

Neural Networks were used in 
11 of the selected studies.
From these, 
only 
4 do not use Long Short-Term Memory (LSTM): 
    Irsoy and Cardie \cite{Irsoy2014720} compares CRF to RNN,
    Zhang et al. \cite{Zhang:2017:NNS:3105654.3105669} describes compact feedforward sequential memory network (cFSMN), 
    Zhao et al. \cite{7953168} describes recurrent convolutional neural network (RCNN) and compares it with LSTM,
    Cui et al. \cite{Cui20162474} describes the hierarchical temporal memory (HTM) sequence memory model, a neural network model that can do continuous online learning, and compares it with LSTM and other techniques.

The remaining 
7 studies use LSTM, where 
    Nguyen et al. \cite{nguyen2018recurrent} compares CRF to some combined models of CRF and BLSTM,
    Xie et al. \cite{7900261} describes fully convolutional recurrent network (FCRN) combining a convolutional network with a multi-layer BLSTM,
    Fang et al. \cite{Fang:2017:DEU:3131672.3131693} describes hierarchical bidirectional deep recurrent neural network (HB-RNN), which uses a tree of BLSTM networks,
    Veli\v{c}kovi\'{c} et al. \cite{Velickovic:2018:CRM:3240925.3240937} describes cross-modal LSTM (X-LSTM), which is a network of LSTMs,
    Tjandra et al. \cite{Tjandra2016448} combines LSTM with Gated Recurrent Unit (GRU) using tensor products,
    Liu et al. \cite{Liu20173671} proposes the global context aware attention LSTM (GCA-LSTM), that adds a global context memory block to LSTM, 
    Feng et al. \cite{feng2018attention} adds a context attention layers to a hierarchical LSTM to produce a Context Attention LSTM (CA-LSTM).

\item   \textbf{In which application areas they are used?}

% On the selected studies, some areas were frequently seen these are some frequently seen areas in which sequence label was applied:
    Part Of Speech (POS) tagging 
    associates part of speech labels to words in a sentence and was studied in 
    \cite{Chatzis20131523} and
    \cite{Tjandra2016448}.
    
    Speech recognition
    converts spoken words into text
    and was studied in 
    \cite{Zhang:2017:NNS:3105654.3105669}, \cite{7953168}, 
    \cite{gravessupervised}, and
    \cite{6423821}.
    
    Handwriting recognition
    digitally transcribes a handwritten text
    and was studied in 
    \cite{Cuong:2014:CRF:2627435.2638567}, 
    \cite{Chatzis20131523}, and
    \cite{7900261}.

    Protein sequence analysis
    compare regions of DNA or RNA with known protein sequences
    and was studied in 
    \cite{dafe2015learning}.
    
    Action recognition
    gives labels to predefined sequences of human motion, commonly translating body postures and hand shapes to commands or words
    and was studied in 
    \cite{6247918},
    % \item video segmentation
    \cite{Chatzis20131523},
    % \item video event classification
    \cite{6213103},
    % \item Gesture recognition:
    \cite{Liu20173671}, and
    % \item sign language
    \cite{Fang:2017:DEU:3131672.3131693}.  
    
    Information extraction
    retrieves structured information from unstructured data, often text,
    and was studied in 
    % \item bibliography extraction
    \cite{Cuong:2014:CRF:2627435.2638567},
    % \item opinion mining
    \cite{Irsoy2014720},
    % \item legal texts
    \cite{nguyen2018recurrent},
    % sentiment classification
    \cite{feng2018attention}, and
    \cite{dafe2015learning}.
    

\item   \textbf{How these algorithms compare to each other?}

% \begin{itemize}
    % \item HMM and CRF: 
    
    As explained by Papadopoulos and Tzanetakis \cite{7579173}, HMM and CRF are both cases of Markov networks, but differ in some aspects.
    % \begin{itemize}
        % \item 
        HMM represents $p(y,x)$ while CRF represents $p(y|x)$ (where $x$ is an observation and $y$ the label), which is why HMM is considered a generative model and CRF a discriminative one.
        Graves \cite{gravessupervised} clarifies that $p(x)$, which can be obtained by $p(x) = \sum_{y} p(x,y)$, can be used to generate artificial data, hence the name.
        % \item 
        HMM assumes strong independence between observation variables while CRF relaxes that assumption.
        % \item 
        CRF generally gives better results than HMM in many real-world problems where the dependence between observation variables is common, like natural language processing, music audio signal processing, speech recognition, and bioinformatics.
        
    % \end{itemize}
    
    % \item High-order CRF:
    
    As Cuong et al. \cite{Cuong:2014:CRF:2627435.2638567} details, the common linear-chain CRF considers dependencies between at most two adjacent labels, because its computational cost is exponential in respect to the number of considered neighbors.
    
    Cuong et al. \cite{Cuong:2014:CRF:2627435.2638567} approach to build high-order dependency relies on label sparsity, which happens when a sequence of repeated labels is a common occurrence.
    
    To achieve higher order dependency, Chatzis and Demiris \cite{Chatzis20131523} combines CRF with a sequence memoizer, which is a non-Markovian model since it can consider all previous sequence values.
    
    % Wang and Zhang \cite{6213103} describes Hidden Conditional Random Fields (HCRF), 
    % Song et al. \cite{6247918} describes Multi-View Latent variable Discriminative Conditional Random Fields (MV-LDCRF),
    % Jeong and Lee \cite{4599397} explain triangular-chain CRF.
    
    
    % \item CRF and neural networks:
    
    Irsoy and Cardie \cite{Irsoy2014720} compared CRF and RNN, obtaining better F1-scores with RNN.
    
    Nguyen et al. \cite{nguyen2018recurrent} compared CRF to some combined models of CRF and BLSTM, obtaining better F1-scores with the BLSTM-CRF models.

    % \item Recurrent neural networks (RNN) and LSTM:
    
    As Graves \cite{gravessupervised} explains, is very difficult for a standard RNN to retain information for longs periods of time because, with time, new data from the input layer tends to prevail over signals from the hidden layer, while LSTM has special `memory cells', which can retain information as long as necessary.
    
    Graves \cite{gravessupervised} compared BLSTM, bidirectional recurrent neural network (BRNN) and multilayer perceptron (MLP). Compared with the other two, BLSTM produced better results in less time.
    
    Zhang et al. \cite{Zhang:2017:NNS:3105654.3105669} 
    proposes a nonrecurrent neural network model with a memory block and obtains better results in less time compared to BLSTM.

    Zhao et al. \cite{7953168} 
    reports that RNNs are hard to train because they cannot take full advantage of GPUs, while convolutional neural networks (CNN) does not improve speech recognition results compared to other techniques.
    Comparing the proposed model RCNN with LSTM, RCNN was faster to train and showed a slightly better result.

% \end{itemize}




% discriminative vs generative classifiers

% segmentation problem

    \item   \textbf{Which of them could produce better results in data carving?}
    
    To answer that question first is necessary to define how the data carving problem compares to the usual problems that sequence labeling solves. In part-of-speech (POS) tagging, a limitation on how far in the sequence an algorithm is allowed to go back has a smaller impact when compared to speech recognition because the later has to solve segmentation, while words are already the desired segments. In speech recognition, each chunk of sound may be a new phoneme, or a continuation of the previous phoneme, or a pause, causing a larger dependency on older sequence items to improve results.
    Regarding the segmentation issue, a data carving algorithm faces a challenge similar to speech recognition, and also protein sequence analysis, for two reasons: its recognition patterns may have variable size and, due to fragmentation, there may exist large distances between related disk sectors holding the content of a deleted file.
    
    Merge comparisons from different studies is a complex task because they naturally use the plain model as a baseline. For example, there are studies achieving better results with high order CRFs instead of CRF, and studies promoting LSTM over CRF, but it is unclear how a higher order CRF compare to LSTM.
    
    Despite that difficulty, there seems to exist a general tendency giving BLSTM an advantage over CRF in speech recognition, while for natural language problems that gap may be smaller and CRF may be a good choice.
    
    While there are studies that reported improvements over some form of general algorithm baseline, it is unwise to use one of these as first attempts while planning a new work. It may happen that the improvements are not achieved when ported to another problem. Or unanticipated implementation issues may arise. Or the solution may not scale well. For that reason, even knowing that there are studies reporting better results than BLSTM, the later still may be the best choice for data carving.
\end{enumerate}
